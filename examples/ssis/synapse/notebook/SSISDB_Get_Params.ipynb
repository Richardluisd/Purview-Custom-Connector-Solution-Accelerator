{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "notebookrun",
              "session_id": 19,
              "statement_id": 39,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-08-26T01:02:54.5195026Z",
              "session_start_time": null,
              "execution_start_time": "2021-08-26T01:02:54.9238761Z",
              "execution_finish_time": "2021-08-26T01:02:54.9239769Z"
            },
            "text/plain": "StatementMeta(notebookrun, 19, 39, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": ""
        }
      ],
      "metadata": {},
      "source": [
        "import sqlite3\r\n",
        "import pyodbc\r\n",
        "import csv\r\n",
        "import pandas as pd\r\n",
        "import datetime\r\n",
        "from azure.storage.blob import BlobClient"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "notebookrun",
              "session_id": 19,
              "statement_id": 40,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-08-26T01:02:54.5924961Z",
              "session_start_time": null,
              "execution_start_time": "2021-08-26T01:02:55.0128538Z",
              "execution_finish_time": "2021-08-26T01:02:55.0129401Z"
            },
            "text/plain": "StatementMeta(notebookrun, 19, 40, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": ""
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true,
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "server = ''\r\n",
        "database = ''\r\n",
        "username = ''\r\n",
        "password = ''\r\n",
        "driver = ''\r\n",
        "blob_file_path = ''\r\n",
        "execution_id_path = ''\r\n",
        "blob_account_name = ''\r\n",
        "blob_account_key = ''\r\n",
        "container = ''\r\n",
        "SSISProject = ''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "notebookrun",
              "session_id": 19,
              "statement_id": 41,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-08-26T01:02:54.6563104Z",
              "session_start_time": null,
              "execution_start_time": "2021-08-26T01:02:55.1094239Z",
              "execution_finish_time": "2021-08-26T01:02:55.6019139Z"
            },
            "text/plain": "StatementMeta(notebookrun, 19, 41, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": ""
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "from pyspark.conf import SparkConf\r\n",
        "from pyspark.sql import SparkSession\r\n",
        "import os\r\n",
        "\r\n",
        "app_name = 'SSISDB_Get_Params'\r\n",
        "my_jars = os.environ.get(\"SPARK_HOME\")\r\n",
        "myconf = SparkConf()\r\n",
        "myconf.setMaster(\"local\").setAppName(\"SSISDB_Get_Params\")\r\n",
        "myconf.set(\"spark.jars\",\"%s/jars/log4j-1.2.17.jar\" % my_jars)\r\n",
        "spark = SparkSession\\\r\n",
        " .builder\\\r\n",
        " .appName(\"SSISDB_Get_Params\")\\\r\n",
        " .config(conf = myconf) \\\r\n",
        " .getOrCreate()\r\n",
        "\r\n",
        "\r\n",
        "Logger= spark._jvm.org.apache.log4j.Logger\r\n",
        "mylogger = Logger.getLogger(app_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "notebookrun",
              "session_id": 19,
              "statement_id": 42,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-08-26T01:02:54.7154516Z",
              "session_start_time": null,
              "execution_start_time": "2021-08-26T01:02:55.691253Z",
              "execution_finish_time": "2021-08-26T01:02:55.84062Z"
            },
            "text/plain": "StatementMeta(notebookrun, 19, 42, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": ""
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "def log_msgs(msg_type,msg):\r\n",
        "        \r\n",
        "        if msg_type.upper() == \"ERROR\":\r\n",
        "            print('ERROR: %s' % repr(msg))\r\n",
        "            mylogger.error(repr(msg))\r\n",
        "        else:\r\n",
        "            print('INFO: %s' % repr(msg))\r\n",
        "            mylogger.info(repr(msg))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "notebookrun",
              "session_id": 19,
              "statement_id": 43,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-08-26T01:02:54.793267Z",
              "session_start_time": null,
              "execution_start_time": "2021-08-26T01:02:55.9294673Z",
              "execution_finish_time": "2021-08-26T01:02:56.0898438Z"
            },
            "text/plain": "StatementMeta(notebookrun, 19, 43, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "INFO: 'Entities Created/Updated'\nINFO: ''\nINFO: ''\nINFO: ''\nINFO: ''\nINFO: ''\nINFO: ''\nINFO: ''\nINFO: ''\nINFO: ''\nINFO: ''\nINFO: ''\nINFO: 'DRIVER=;SERVER=;DATABASE=;UID=;PWD='"
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "log_msgs(\"INFO\",'Entities Created/Updated')\r\n",
        "log_msgs(\"INFO\", server)\r\n",
        "log_msgs(\"INFO\",database)\r\n",
        "log_msgs(\"INFO\",username)\r\n",
        "log_msgs(\"INFO\",password)\r\n",
        "log_msgs(\"INFO\",driver)\r\n",
        "log_msgs(\"INFO\",blob_file_path)\r\n",
        "log_msgs(\"INFO\",execution_id_path)\r\n",
        "log_msgs(\"INFO\",blob_account_name)\r\n",
        "log_msgs(\"INFO\",blob_account_key)\r\n",
        "log_msgs(\"INFO\",container)\r\n",
        "log_msgs(\"INFO\",SSISProject)\r\n",
        "log_msgs(\"INFO\",'DRIVER='+f'{driver}'+';SERVER='+f'{server}'+';DATABASE='+f'{database}'+';UID='+f'{username}'+';PWD='+ f'{password}')\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "notebookrun",
              "session_id": 19,
              "statement_id": 44,
              "state": "finished",
              "livy_statement_state": "cancelled",
              "queued_time": "2021-08-26T01:02:54.91281Z",
              "session_start_time": null,
              "execution_start_time": "2021-08-26T01:02:56.1818556Z",
              "execution_finish_time": "2021-08-26T01:03:27.445991Z"
            },
            "text/plain": "StatementMeta(notebookrun, 19, 44, Finished, Cancelled)"
          },
          "metadata": {}
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "blob_connection_string = 'DefaultEndpointsProtocol=https;AccountName=' + blob_account_name + ';AccountKey=' + blob_account_key + \\\r\n",
        "';EndpointSuffix=core.windows.net'\r\n",
        "\r\n",
        "execution_id_url = 'abfss://' + container + '@' + blob_account_name + '.dfs.core.windows.net/' + execution_id_path\r\n",
        "\r\n",
        "PARAM_Query = \\\r\n",
        "\"SELECT CAST(a.[parameter_name] as nvarchar(max)) as parameter_name,CAST(a.parameter_value as nvarchar(max)) as parameter_value  \\\r\n",
        "FROM [SSISDB].[internal].[execution_parameter_values] a \\\r\n",
        "INNER JOIN [SSISDB].[internal].[executions] b \\\r\n",
        "ON ( a.execution_id = b.execution_id ) \\\r\n",
        "WHERE [object_type] = 30 and a.[execution_id] = \\\r\n",
        "(SELECT max([execution_id]) \\\r\n",
        "from [SSISDB].[internal].[execution_parameter_values]) \\\r\n",
        "AND a.[parameter_value] IS NOT NULL \\\r\n",
        "AND a.[parameter_value] != 0 \\\r\n",
        "AND b.[project_name] = '\" + f'{SSISProject}' + \"' \\\r\n",
        "GROUP BY a.[object_type],a.[parameter_name],a.[parameter_value],b.[project_name],a.execution_id\"\r\n",
        "\r\n",
        "connection = pyodbc.connect('DRIVER='+f'{driver}'+';SERVER='+f'{server}'+';DATABASE='+f'{database}'+';UID='+f'{username}'+';PWD='+ f'{password}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "notebookrun",
              "session_id": 19,
              "statement_id": 45,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-08-26T01:02:55.044096Z",
              "session_start_time": null,
              "execution_start_time": "2021-08-26T01:03:27.5401346Z",
              "execution_finish_time": "2021-08-26T01:03:27.6873793Z"
            },
            "text/plain": "StatementMeta(notebookrun, 19, 45, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": ""
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "Get_execution_id = \"SELECT CAST(MAX(a.execution_id) as int) as execution_id FROM [SSISDB].[internal].[execution_parameter_values] a\"\r\n",
        "cursor = connection.cursor()\r\n",
        "cursor.execute(Get_execution_id)\r\n",
        "rows_check = cursor.fetchall()\r\n",
        "columns_check = [column[0] for column in cursor.description]\r\n",
        "if rows_check[0][0] is None:\r\n",
        "    New_Execution_Id = 0\r\n",
        "else:\r\n",
        "    New_Execution_Id = int(rows_check[0][0]) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "notebookrun",
              "session_id": 19,
              "statement_id": 46,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-08-26T01:02:55.133801Z",
              "session_start_time": null,
              "execution_start_time": "2021-08-26T01:03:27.7800084Z",
              "execution_finish_time": "2021-08-26T01:03:59.0657852Z"
            },
            "text/plain": "StatementMeta(notebookrun, 19, 46, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "OperationalError",
          "evalue": "('HYT00', '[HYT00] [Microsoft][ODBC Driver 17 for SQL Server]Login timeout expired (0) (SQLDriverConnect)')",
          "traceback": [
            "OperationalError: ('HYT00', '[HYT00] [Microsoft][ODBC Driver 17 for SQL Server]Login timeout expired (0) (SQLDriverConnect)')",
            "Traceback (most recent call last):\n",
            "pyodbc.OperationalError: ('HYT00', '[HYT00] [Microsoft][ODBC Driver 17 for SQL Server]Login timeout expired (0) (SQLDriverConnect)')\n"
          ]
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "Old_Execution_Id = None\r\n",
        "if New_Execution_Id != 0:\r\n",
        "    try:\r\n",
        "        df = spark.read.load(execution_id_url, format='csv', header=True).toPandas()\r\n",
        "        for index, row in df.iterrows():\r\n",
        "            row.execution_id\r\n",
        "            Old_Execution_Id = int(row.execution_id)\r\n",
        "    except:\r\n",
        "        pass\r\n",
        "else:\r\n",
        "    df_Execution_Id = pd.DataFrame.from_records(rows_check, columns=columns_check)\r\n",
        "    print(df_Execution_Id)\r\n",
        "    if df_Execution_Id['execution_id'][0] is None:\r\n",
        "        Old_Execution_Id = None\r\n",
        "        New_Execution_Id = None\r\n",
        "    else:\r\n",
        "        df_sp_df_Execution_Id = spark.createDataFrame(df_Execution_Id)\r\n",
        "\r\n",
        "        df_sp_df_Execution_Id.write.option('header', 'true').mode('overwrite').csv(execution_id_url)\r\n",
        "        Old_Execution_Id = 0\r\n",
        "if (Old_Execution_Id is None and New_Execution_Id is not None) or (Old_Execution_Id < New_Execution_Id):\r\n",
        "    connection = pyodbc.connect('DRIVER='+f'{driver}'+';SERVER='+f'{server}'+';DATABASE='+f'{database}'+';UID='+f'{username}'+';PWD='+ f'{password}')\r\n",
        "    cursor = connection.cursor()\r\n",
        "    # Execute the query\r\n",
        "    cursor.execute(PARAM_Query)\r\n",
        "    rows = cursor.fetchall()\r\n",
        "    # # get column names from cursor\r\n",
        "    columns = [column[0] for column in cursor.description]\r\n",
        "    df_Params = pd.DataFrame.from_records(rows, columns=columns)\r\n",
        "    csvdata = df_Params.to_csv('Params.txt')\r\n",
        "    blob = BlobClient.from_connection_string(conn_str=f'{blob_connection_string}', container_name=f'{container}', blob_name=f'{blob_file_path}')\r\n",
        "    with open(\"Params.txt\", \"rb\") as data:\r\n",
        "       blob.upload_blob(data,overwrite=True)\r\n",
        "   \r\n",
        "    df_Execution_Id = pd.DataFrame.from_records(rows_check, columns=columns_check)\r\n",
        "    df_sp_df_Execution_Id = spark.createDataFrame(df_Execution_Id)\r\n",
        "    df_sp_df_Execution_Id.write.option('header', 'true').mode('overwrite').csv(execution_id_url)\r\n",
        "    mssparkutils.notebook.exit(True) \r\n",
        "else: \r\n",
        "    mssparkutils.notebook.exit(False)  \r\n",
        ""
      ]
    }
  ],
  "metadata": {
    "save_output": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  }
}